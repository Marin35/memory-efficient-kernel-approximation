{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils.py\n",
    "\n",
    "First, let's take a look at utils.py, there are many functions which are useful.\n",
    "For example, the gaussian kernel implementation and the matrix kernel basic calculus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel(gamma, x_1, x_2):\n",
    "    \"\"\"\n",
    "    Compute the gaussian kernel between two vectors.\n",
    "\n",
    "    :param gamma: float, gamma value\n",
    "    :param x_1: array\n",
    "    :param x_2: array\n",
    "    :returns: float, the kernel value\n",
    "    \"\"\"\n",
    "    return np.exp((-1) * gamma * np.linalg.norm(x_1 - x_2)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kernel_matrix(dataframe, gamma):\n",
    "    \"\"\"\n",
    "    Take dataframe and compute the kernel matrix associated.\n",
    "    :param dataframe: a df which size is (n x d)\n",
    "    :param gamma: float, parameter for the shift of the gaussian kernel\n",
    "    :return: matrix, the kernel matrix (of size n x n)\n",
    "    \"\"\"\n",
    "    n = len(dataframe)\n",
    "    kernel_matrix = np.zeros((n, n))  # Initialize the kernel matrix\n",
    "    for i in range(n):\n",
    "        for j in range(0, i):\n",
    "            kernel_matrix[i, j] = gaussian_kernel(gamma, dataframe.iloc[i], dataframe.iloc[j])\n",
    "\n",
    "    # Then we do a transposition because the kernel matrix is symetric\n",
    "    return kernel_matrix + kernel_matrix.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then here is the function for the low-rank pseudo-inverse approximation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def low_rank_approx(matrix, rank):\n",
    "    \"\"\"\n",
    "    Compute the pseudo-inverse of a low k-rank approximation of a matrix.\n",
    "    :param matrix: matrix to get the approximation\n",
    "    :param rank: int, the rank of the matrix approximation\n",
    "    :returns: a matrix which is the pseudo-inverse of the approximation\n",
    "    \"\"\"\n",
    "\n",
    "    u, s, v = np.linalg.svd(matrix, full_matrices=False)\n",
    "\n",
    "    M_k = np.zeros((len(u), len(v)))\n",
    "    for i in range(rank):\n",
    "        M_k += s[i] * np.outer(u.T[i], v[i])\n",
    "    pseudo_inverse_M_k = np.linalg.pinv(M_k)\n",
    "    return pseudo_inverse_M_k\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the function to get the decomposition for the W vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_decomposition(sample, df, gamma):\n",
    "    \"\"\"\n",
    "    Compute the decomposition of the matrix W.\n",
    "    :param sample: matrix of size m x d\n",
    "    :param df: matrix of size n x d\n",
    "    :param gamma: float, parameter for the gaussian shift\n",
    "    :return: The W matrix of size n x m\n",
    "    \"\"\"\n",
    "    number_sample_m = len(sample)\n",
    "    W = np.zeros((len(df), number_sample_m))  # Create the matrix\n",
    "    for j in range(number_sample_m):\n",
    "        for i in range(len(df)):\n",
    "            W[i, j] = gaussian_kernel(gamma, sample.iloc[j], df.iloc[i])\n",
    "\n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can perform the Nystrom approximation everything matrix, we have to indicate the rank and the size of the sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nystorm_approximation(df, number_sample_m, rank_k, gamma):\n",
    "    \"\"\"\n",
    "    Compute the Nystorm approximation of the dataframe.\n",
    "\n",
    "    :param df: The dataframe which has the data points.\n",
    "    :param number_sample_m: The size of the sample to take.\n",
    "    :param rank_k: The rank of the matrix for the approximation.\n",
    "    :param gamma: float, the parameter for the shift\n",
    "    :return: W (matrix of size n x m) and L (matrix of size m x m)\n",
    "    \"\"\"\n",
    "\n",
    "    sample = df.sample(n=number_sample_m, random_state=0, axis=0)\n",
    "\n",
    "    kernel_matrix = get_kernel_matrix(sample, gamma)  # Compute the kernel matrix\n",
    "    M_k = low_rank_approx(kernel_matrix, rank_k)  # Get the pseudo-inverse of the low rank approximation\n",
    "    L = M_k\n",
    "    W = get_decomposition(sample, df, gamma)  # Get the vector of the decomposition\n",
    "\n",
    "    return W, L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# meka.py\n",
    "\n",
    "This file is composed of the class of the meka algorithm, we initiate it with the following parameters. We are going to store each block of W and L in an array and a dictionnary with corresponding indexes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __init__(self, df, gamma, rank_k, number_clusters_c, number_sample_m):\n",
    "        \"\"\"\n",
    "        Initialize the algorithm.\n",
    "        :param df: Dataframe with the values\n",
    "        :param gamma: float, parameter for the gaussian shift.\n",
    "        :param rank_k: int, rank of the low-rank approximation matrix.\n",
    "        :param number_clusters_c: int, number of blocks (clusters)\n",
    "        :param number_sample_m: int, size of the sample for the decomposition.\n",
    "        :return: G_hat, matrix based on the MEKA algorith\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.gamma = gamma\n",
    "        self.rank_k = rank_k\n",
    "        self.number_clusters_c = number_clusters_c\n",
    "        self.number_sample_m = number_sample_m\n",
    "\n",
    "        self.blocks_of_L = {}\n",
    "        self.blocks_of_W = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already performed the K-means partitioning, then we extract the dataframe corresponding to each cluster and we do a Nystorm approximation of it based on the function in utils.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def compute_on_diagonal_blocks(self):\n",
    "        \"\"\"\n",
    "        Compute the on-diagonal blocks by using the nystorm approximation for each block.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        for cluster in range(self.number_clusters_c):\n",
    "            df_cluster = self.df.loc[self.df['clusters'] == cluster]  # Extract the cluster\n",
    "            df_cluster = df_cluster.iloc[:, :-1]  # Delete the last column\n",
    "\n",
    "            W, L = nystorm_approximation(df_cluster, self.number_sample_m, self.rank_k, self.gamma)\n",
    "\n",
    "            # We can get the block G(s,s) by doing:\n",
    "            # G = W * M_k * transpose(W)\n",
    "\n",
    "            self.blocks_of_W.append(W)\n",
    "            position = str(cluster) + '_' + str(cluster)\n",
    "            self.blocks_of_L[position] = L  # Store the L(s,s) element\n",
    "\n",
    "            print(\"Success for the cluster \" + str(cluster))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can compute the second step, we randomly extract a submatrix \\hat{G} and we get the value of the vector L  based on the least square solution of the equation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def compute_off_diagonal_blocks(self):\n",
    "        \"\"\"\n",
    "        Compute the off-diagonal blocks. We use the blocks of W calculated previously.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        dim_subsample = 3 * self.rank_k  # Get the size of the subsample matrix\n",
    "        for s in range(self.number_clusters_c):\n",
    "            for t in range(self.number_clusters_c):\n",
    "                if s != t:\n",
    "\n",
    "                    df_s = self.df.loc[self.df['clusters'] == s].iloc[:, :-1]\n",
    "                    df_t = self.df.loc[self.df['clusters'] == t].iloc[:, :-1]\n",
    "\n",
    "                    number_col_s = df_s.shape[0]\n",
    "                    number_row_t = df_t.shape[0]\n",
    "\n",
    "                    index_col_s = random.randint(0, number_col_s - dim_subsample)\n",
    "                    index_row_t = random.randint(0, number_row_t - dim_subsample)\n",
    "\n",
    "                    # We have the coordinates of the submatrix\n",
    "\n",
    "                    sub_matrix = np.zeros((dim_subsample, dim_subsample))\n",
    "\n",
    "                    for i in range(index_row_t, index_row_t + dim_subsample):\n",
    "                        for j in range(index_col_s, index_col_s + dim_subsample):\n",
    "                            sub_matrix[i - index_row_t, j - index_col_s] = gaussian_kernel(self.gamma, df_s.iloc[j],\n",
    "                                                                                           df_t.iloc[i])\n",
    "\n",
    "                    # Then we can extract W_nu_s and W_nu_t\n",
    "\n",
    "                    W_s = self.blocks_of_W[s]\n",
    "                    W_nu_s = W_s[index_col_s:index_col_s + dim_subsample, :]\n",
    "\n",
    "                    W_t = self.blocks_of_W[t]\n",
    "                    W_nu_t = W_t[index_row_t:index_row_t + dim_subsample, :]\n",
    "\n",
    "                    # And finally we compute the dot product (i.e solve Least Squares) to get L(s,t)\n",
    "\n",
    "                    L_s_t = inv(np.transpose(W_nu_s).dot(W_nu_s)).dot(np.transpose(W_nu_s)).dot(sub_matrix).dot(\n",
    "                        W_nu_t).dot(\n",
    "                        inv(np.transpose(W_nu_t).dot(W_nu_t)))\n",
    "\n",
    "                    self.blocks_of_L[str(s) + '_' + str(t)] = L_s_t\n",
    "\n",
    "                    # We have now all the blocks of L and W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now calculated all the blocks of L and W. By doing a block product of matrices, we can reconstruct G hat,\n",
    "we have to be careful about the indexes value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def reconstruct_G_hat(self):\n",
    "        \"\"\"\n",
    "        Reconstruct G hat based on the blocks decomposition.\n",
    "        :return: the matrix G_hat of size (n x n)\n",
    "        \"\"\"\n",
    "        n = len(self.df)\n",
    "        G_hat = np.zeros((n, n))\n",
    "\n",
    "        index_row = 0\n",
    "        for i in range(self.number_clusters_c):\n",
    "            W_1 = self.blocks_of_W[i]\n",
    "            index_col = 0\n",
    "            for j in range(self.number_clusters_c):\n",
    "                W_2 = np.transpose(self.blocks_of_W[j])\n",
    "                L = self.blocks_of_L[str(i) + '_' + str(j)]\n",
    "                G_partial = W_1.dot(L).dot(W_2)\n",
    "                a, b = G_partial.shape\n",
    "                G_hat[index_row:index_row + a, index_col:index_col + b] = G_partial\n",
    "\n",
    "                index_col += b\n",
    "            index_row += a\n",
    "\n",
    "        print('Construction of G_hat is finished.')\n",
    "        return G_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main.py\n",
    "\n",
    "Here is the main file. We get data from ijcnn1, we randomly sample 20 000 data points from it then we create the real G (kernel matrix), we compute a G_nystorm matrix thanks to the algorithm in utils.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/ijcnn1.csv', sep=',').iloc[:, 1:]\n",
    "df = df.sample(n=20000, random_state=0)\n",
    "rank_k = 128\n",
    "number_sample_m = 300\n",
    "gamma = 1\n",
    "\n",
    "G = get_kernel_matrix(df, gamma=gamma)\n",
    "\n",
    "\n",
    "W, L = nystorm_approximation(df, number_sample_m, rank_k, gamma)\n",
    "G_nystrom = W.dot(L).dot(np.transpose(W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we do a k-means clustering and perform the MEKA algorithm to get the third matrix G_hat based on the MEKA algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_clusters_c = 10\n",
    "# Do the Kmeans clustering and group data points based on that\n",
    "kmeans = KMeans(n_clusters=number_clusters_c, random_state=0).fit(df)\n",
    "\n",
    "df['clusters'] = kmeans.labels_\n",
    "df_c = df.sort_values(by='clusters')\n",
    "df = df.drop(['clusters'], axis=1)\n",
    "\n",
    "meka_algorithm = Meka(df_c, gamma, rank_k, number_clusters_c, number_sample_m=20)\n",
    "meka_algorithm.compute_on_diagonal_blocks()\n",
    "meka_algorithm.compute_off_diagonal_blocks()\n",
    "\n",
    "G_hat = meka_algorithm.reconstruct_G_hat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the following score which evaluate the approximation of the kernel matrix based on the Froebenius norm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = np.linalg.norm(G - G_nystrom, 'fro') / np.linalg.norm(G, 'fro')\n",
    "\n",
    "score_2 = np.linalg.norm(G - G_hat, 'fro') / np.linalg.norm(G, 'fro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately my computer was not able to compute the kernel matrix with 20 000 sample data points but when I take 2000 data points and adapt the parameters accordingly I'm able to obtain a score of 0.29 for the normal Nystrom method and around 0.18 for the MEKA algorithm.\n",
    "\n",
    "\n",
    "BOUTHEMY Marin, ENSAE 3A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
